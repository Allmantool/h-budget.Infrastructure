name: monitoring
services:
  prometheus:
    container_name: prometheus
    image: prom/prometheus:v3.9.1
    labels:
      project: monitoring
      service.name: prometheus
      com.docker.compose.project: monitoring
      com.docker.compose.service: prometheus
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --web.enable-remote-write-receiver
      - --enable-feature=exemplar-storage
      - --enable-feature=native-histograms
    networks:
      - rates-api-network
      - accounting-api-network
      - mongo-network
      - kafka-net
      - mssql-network
      - redis-network
      - ui-network
      - event-store-db-net
      - shared-monitoring-net
    ports:
      - 9190:9090
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ${PROMETHEUS_CONFIG}:/etc/prometheus/prometheus.yml
      # - ${PROMETHEUS_DATA}:/prometheus
      - prometheus-data:/prometheus
    depends_on:
      - homebudget-rates-api
      - homebudget-accounting-api
      - gateway-api
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "0.2"
          memory: "726m"

  tempo:
    image: grafana/tempo:2.10.1
    container_name: gf-tempo
    labels:
      project: monitoring
      service.name: gf-tempo
      com.docker.compose.project: monitoring
      com.docker.compose.service: gf-tempo
    ports:
      - 3200:3200
    volumes:
      - tempo-data:/var/tempo
      - ${GF_TEMPO_CONFIG}:/etc/tempo.yaml
    command: ["--config.file=/etc/tempo.yaml"]
    networks:
      - rates-api-network
      - accounting-api-network
      - mongo-network
      - kafka-net
      - mssql-network
      - redis-network
      - ui-network
      - shared-monitoring-net
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "0.3"
          memory: "512mb"

  grafana:
    container_name: grafana
    image: grafana/grafana:12.3.3-ubuntu
    labels:
      project: monitoring
      service.name: grafana
      com.docker.compose.project: monitoring
      com.docker.compose.service: grafana
    networks:
      - rates-api-network
      - accounting-api-network
      - mongo-network
      - kafka-net
      - mssql-network
      - redis-network
      - ui-network
      - shared-monitoring-net
    restart: unless-stopped
    environment:
      GF_FEATURE_TOGGLES_ENABLE: traceqlEditor
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
      GF_SERVER_DOMAIN: grafana.${HOME_SERVER}
      GF_SERVER_ROOT_URL: https://${HOME_SERVER}/
      GF_SERVER_SERVE_FROM_SUB_PATH: "false"
      GF_SERVER_PROTOCOL: http
      GF_PLUGINS_PREINSTALL: "https://github.com/raintank/crate-datasource/archive/master.zip;crate-datasource,grafana-clock-panel,natel-plotly-panel,grafana-exploretraces-app"
    ports:
      - 3500:3000
    volumes:
      - ${GRAFANA_CONFIG_FOLDER}:/etc/grafana/provisioning
      - gf-logs:/var/log/grafana
      - gf-data:/var/lib/grafana
      # - gf-provisioning:/etc/grafana/provisioning

      # - ${GRAFANA_CONFIG}:/etc/grafana/grafana-provisioning.yml
      # - ${GRAFANA_CONFIG}:/etc/grafana/provisioning/datasources/datasources.yaml

      # - ${GF_PATHS_LOGS}:/var/log/grafana
      # - ${GF_STORAGE}:/var/lib/grafana
      # - ${GF_PATHS_PROVISIONING}:/etc/grafana/provisioning
    depends_on:
      - prometheus
      - loki
      - tempo
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "0.5"
          memory: "1.5g"

  loki:
    image: grafana/loki:3.6.6
    container_name: gf-loki
    labels:
      project: monitoring
      service.name: gf-loki
      com.docker.compose.project: monitoring
      com.docker.compose.service: gf-loki
    user: "10001:10001"
    networks:
      - rates-api-network
      - accounting-api-network
      - mongo-network
      - kafka-net
      - mssql-network
      - redis-network
      - ui-network
      - shared-monitoring-net
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/config/loki.yaml
    volumes:
      - ${GF_LOKI_CONFIG}:/etc/loki/config/loki.yaml
      # - loki-data:/tmp/
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "0.5"
          memory: "1g"

  alloy:
    image: grafana/alloy:v1.13.1
    container_name: gf-alloy
    labels:
      project: monitoring
      service.name: gf-alloy
      com.docker.compose.project: monitoring
      com.docker.compose.service: gf-alloy
    environment:
      - HOSTNAME=${HOSTNAME}
    command:
      - run
      - --server.http.listen-addr=0.0.0.0:12345
      - --storage.path=/var/lib/alloy/data
      - /etc/alloy/config.alloy
    ports:
      - "12345:12345" # Alloy metrics
      - "4317:4317" # OTLP gRPC
      - "4318:4318" # OTLP HTTP
    volumes:
      - ${GF_ALLOY_CONFIG}:/etc/alloy/config.alloy
      - /var/run/docker.sock:/var/run/docker.sock:ro # For Docker discovery
      - alloy-data:/var/lib/alloy/data
      # - ./logs:/var/log/alloy  # Optional: For Alloy logs
    networks:
      - rates-api-network
      - accounting-api-network
      - mongo-network
      - kafka-net
      - mssql-network
      - redis-network
      - ui-network
      - shared-monitoring-net
    depends_on:
      - tempo
      - loki
      - prometheus
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "0.2"
          memory: "512mb"
    restart: unless-stopped

  cadvisor:
    image: ghcr.io/google/cadvisor:latest #v0.56.2
    container_name: cadvisor
    labels:
      project: monitoring
      service.name: cadvisor
      com.docker.compose.project: monitoring
      com.docker.compose.service: cadvisor
    privileged: true
    # devices:
    #   - /dev/kmsg:/dev/kmsg
    environment:
      - DOCKER_API_VERSION=1.52
    command: >
      - '--docker_only=false'
      - '--housekeeping_interval=2m'
      - '--max_housekeeping_interval=2m'
      - '--disable_metrics=hugetlb,sched,perf_event,percpu,tcp,udp,process,oom,referenced_memory,cpu_topology,cpuset,resctrl,memory_numa,cpu_load,irq,accelerators'
      - '--event_storage_event_limit=500'
      - '--event_storage_age_limit=30m'
      - '--containerd=/run/containerd/containerd.sock'
    volumes:
      # - /:/rootfs:ro
      # - /var/run:/var/run:ro
      # - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      # - /dev/disk/:/dev/disk:ro
      - /run/containerd/containerd.sock:/run/containerd/containerd.sock:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /sys/fs/cgroup:/sys/fs/cgroup:ro
      - /proc:/host/proc:ro
    ports:
      - "8195:8080"
    networks:
      - shared-monitoring-net
    restart: unless-stopped
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "0.75"
          memory: 725M
        reservations:
          cpus: "0.25"
          memory: 128M

  node-exporter-mount-init:
    image: alpine:3.23
    container_name: node-exporter-mount-init
    user: root
    restart: "no"
    privileged: true
    volumes:
      - /:/host:rw
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Attempting to make / rshared for Node Exporter..."
        mount --make-rshared /host && echo "/ is now rshared"
        echo "Init complete"
    labels:
      role: "node-exporter-mount-init"

  node-exporter:
    image: prom/node-exporter:v1.10.2
    container_name: node-exporter
    user: root
    labels:
      project: monitoring
      service.name: node-exporter
      com.docker.compose.project: monitoring
      com.docker.compose.service: node-exporter
    pid: host
    depends_on:
      - node-exporter-mount-init
    command:
      - "--path.rootfs=/host"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($|/)"
      - "--collector.filesystem.fs-types-exclude=^(sysfs|procfs|autofs|cgroup|overlay|nsfs|tmpfs|devtmpfs)$"
      - "--collector.cpu"
      - "--collector.loadavg"
      - "--collector.meminfo"
      - "--collector.diskstats"
      # - "--collector.netclass"
      - "--collector.netdev"
      # - "--collector.processes"
      # - "--collector.systemd"
      - "--no-collector.hwmon"
      - "--collector.disable-defaults"
    volumes:
      # - "${DOCKER_NODE_PATHS_LOGS}:/host:ro,rslave"
      - /:/host:ro,rslave
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      # - /:/rootfs:ro
    ports:
      - "9100:9100"
    networks:
      - shared-monitoring-net
    restart: unless-stopped
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "0.2"
          memory: "128m"

  mongo-db-exporter:
    image: percona/mongodb_exporter:2.37
    container_name: prometheus-mongo-db-exporter
    labels:
      project: monitoring
      service.name: prometheus-mongo-db-exporter
      com.docker.compose.project: monitoring
      com.docker.compose.service: prometheus-mongo-db-exporter
    command:
      - "--mongodb.global-conn-pool"
      - "--collector.diagnosticdata"
      - "--discovering-mode"
      - "--compatible-mode"
      - "--mongodb.collstats-colls=admin.companies,admin.restaurants"
      - "--collect-all"
    environment:
      - MONGODB_URI=mongodb://mongoadmin:${MONGO_DB_PASSWORD}@shared-mongo-db:27017/?maxPoolSize=1000&waitQueueTimeoutMS=15000
    restart: unless-stopped
    ports:
      - "9216:9216"
      - "17001:17001"
    networks:
      - mongo-network
      - shared-monitoring-net
    depends_on:
      - prometheus
      - shared-mongo-db
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "1"
          memory: "128m"

  kafka-exporter:
    image: danielqsj/kafka-exporter:v1.9.0
    container_name: prometheus-kafka-exporter
    labels:
      project: monitoring
      service.name: prometheus-kafka-exporter
      com.docker.compose.project: monitoring
      com.docker.compose.service: prometheus-kafka-exporter
    command:
      - "--kafka.server=kafka:29092"
    ports:
      - 9308:9308
    networks:
      - kafka-net
      - shared-monitoring-net
    depends_on:
      - prometheus
      - kafka
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 30
        delay: 15s
        window: 1080s
      resources:
        limits:
          cpus: "1"
          memory: "128m"

  mssql-exporter:
    image: allmantool/sql-monitoring-exporter:0.0.8
    container_name: prometheus-mssql-exporter
    labels:
      project: monitoring
      service.name: prometheus-mssql-exporter
      com.docker.compose.project: monitoring
      com.docker.compose.service: prometheus-mssql-exporter
    environment:
      SERVER: ${SQL_SERVER}
      PORT: 1433
      USERNAME: SA
      PASSWORD: ${SQL_PASSWORD}
    ports:
      - 4000:4000
    networks:
      - mssql-network
      - shared-monitoring-net
    depends_on:
      - prometheus
      - ${SQL_SERVER}
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "0.5"
          memory: "256m"

  redis-exporter:
    image: oliver006/redis_exporter:v1.81.0
    container_name: prometheus-redis-exporter
    labels:
      project: monitoring
      service.name: prometheus-redis-exporter
      com.docker.compose.project: monitoring
      com.docker.compose.service: prometheus-redis-exporter
    environment:
      REDIS_ADDR: redis_server:6379
    ports:
      - 9121:9121
    networks:
      - redis-network
      - shared-monitoring-net
    depends_on:
      - prometheus
      - redis_server
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "1"
          memory: "128m"

  nginx-exporter:
    image: nginx/nginx-prometheus-exporter:1.5.1
    container_name: nginx-exporter
    labels:
      project: monitoring
      service.name: nginx-exporter
      com.docker.compose.project: monitoring
      com.docker.compose.service: nginx-exporter
    command:
      - "--nginx.scrape-uri=http://localhost:5407/stub_status"
    ports:
      - 9113:9113
    networks:
      - ui-network
    depends_on:
      - prometheus
      - homebudget-ui
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "1"
          memory: "128m"

  event-store-db-exporter:
    image: marcinbudny/eventstore_exporter:0.17.0
    container_name: event-store-db-exporter
    labels:
      project: monitoring
      service.name: event-store-db-exporter
      com.docker.compose.project: monitoring
      com.docker.compose.service: event-store-db-exporter
    environment:
      - EVENTSTORE_URL=http://event-store-db:2113
      - EVENTSTORE_USER=admin
      - EVENTSTORE_PASSWORD=changeit
      - ENABLE_PARKED_MESSAGES_STATS=True
    ports:
      - 9448:9448
    networks:
      - event-store-db-net
      - shared-monitoring-net
    depends_on:
      - prometheus
      - event-store-db
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "1"
          memory: "128m"
