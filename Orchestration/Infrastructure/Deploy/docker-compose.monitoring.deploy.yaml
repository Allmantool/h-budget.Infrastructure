name: monitoring
services:
  prometheus:
    container_name: prometheus
    image: prom/prometheus:v3.8.1
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--web.enable-remote-write-receiver"
    networks:
      - rates-api-network
      - accounting-api-network
      - mongo-network
      - kafka-net
      - mssql-network
      - redis-network
      - ui-network
      - event-store-db-net
      - shared-monitoring-net
    ports:
      - 9090:9090
    volumes:
      - ${PROMETHEUS_CONFIG}:/etc/prometheus/prometheus.yml
      # - ${PROMETHEUS_DATA}:/prometheus
      - prometheus-data:/prometheus
    depends_on:
      - homebudget-rates-api
      - homebudget-accounting-api
      - gateway-api
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "0.5"
          memory: "726m"

  tempo:
    image: grafana/tempo:2.9.0
    container_name: gf-tempo
    ports:
      - "3200:3200"
    volumes:
      - tempo-data:/var/tempo
      - ${GF_TEMPO_CONFIG}:/etc/tempo.yaml
    command: ["--config.file=/etc/tempo.yaml"]
    networks:
      - rates-api-network
      - accounting-api-network
      - mongo-network
      - kafka-net
      - mssql-network
      - redis-network
      - ui-network
      - shared-monitoring-net
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "1"
          memory: "512m"

  grafana:
    container_name: grafana
    image: grafana/grafana:12.3.1-ubuntu
    networks:
      - rates-api-network
      - accounting-api-network
      - mongo-network
      - kafka-net
      - mssql-network
      - redis-network
      - ui-network
      - shared-monitoring-net
    restart: unless-stopped
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
      GF_SERVER_DOMAIN: grafana.${HOME_SERVER}
      GF_SERVER_ROOT_URL: https://${HOME_SERVER}/
      GF_SERVER_SERVE_FROM_SUB_PATH: "false"
      GF_SERVER_PROTOCOL: http
      GF_INSTALL_PLUGINS: "https://github.com/raintank/crate-datasource/archive/master.zip;crate-datasource,grafana-clock-panel,grafana-worldmap-panel,natel-plotly-panel"
    ports:
      - 3000:3000
    volumes:
      - ${GRAFANA_CONFIG_FOLDER}:/etc/grafana/provisioning
      - gf-logs:/var/log/grafana
      - gf-data:/var/lib/grafana
      # - gf-provisioning:/etc/grafana/provisioning

      # - ${GRAFANA_CONFIG}:/etc/grafana/grafana-provisioning.yml
      # - ${GRAFANA_CONFIG}:/etc/grafana/provisioning/datasources/datasources.yaml

      # - ${GF_PATHS_LOGS}:/var/log/grafana
      # - ${GF_STORAGE}:/var/lib/grafana
      # - ${GF_PATHS_PROVISIONING}:/etc/grafana/provisioning
    depends_on:
      - prometheus
      - loki
      - tempo
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "1"
          memory: "1g"

  loki:
    image: grafana/loki:3.6.3
    container_name: gf-loki
    user: "10001:10001"
    networks:
      - rates-api-network
      - accounting-api-network
      - mongo-network
      - kafka-net
      - mssql-network
      - redis-network
      - ui-network
      - shared-monitoring-net
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/config/loki.yaml
    volumes:
      - ${GF_LOKI_CONFIG}:/etc/loki/config/loki.yaml
      # - loki-data:/tmp/
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "1"
          memory: "1024m"

  alloy:
    image: grafana/alloy:v1.12.1
    container_name: gf-alloy
    command:
      - run
      - --server.http.listen-addr=0.0.0.0:12345
      - --storage.path=/var/lib/alloy/data
      - /etc/alloy/config.alloy
    ports:
      - "12345:12345" # Alloy metrics
      - "4317:4317" # OTLP gRPC (can be used as alternative to Tempo's port)
      - "4318:4318" # OTLP HTTP (can be used as alternative to Tempo's port)
    volumes:
      - ${GF_ALLOY_CONFIG}:/etc/alloy/config.alloy
      - /var/run/docker.sock:/var/run/docker.sock:ro # For Docker discovery
      - alloy-data:/var/lib/alloy/data
      # - ./logs:/var/log/alloy  # Optional: For Alloy logs
    networks:
      - rates-api-network
      - accounting-api-network
      - mongo-network
      - kafka-net
      - mssql-network
      - redis-network
      - ui-network
      - shared-monitoring-net
    depends_on:
      - tempo
      - loki
      - prometheus
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "1"
          memory: "726m"
    restart: unless-stopped

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.55.1
    container_name: cadvisor
    privileged: true
    environment:
      - DOCKER_API_VERSION=1.52
    command:
      - "--containerd=/run/containerd/containerd.sock"
      - "--docker_only=false"
      - "--housekeeping_interval=10s"
    volumes:
      - /dev/disk/:/dev/disk:ro
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /sys/fs/cgroup:/sys/fs/cgroup:ro
      - /var/lib/docker:/var/lib/docker:ro
      - /run/containerd/containerd.sock:/run/containerd/containerd.sock:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro # Mount Docker socket to cAdvisor
    ports:
      - "8080:8080"
    networks:
      - shared-monitoring-net
    restart: unless-stopped
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "2"
          memory: "726m"

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    pid: host
    command:
      - "--path.rootfs=/host"
    volumes:
      - "${DOCKER_NODE_PATHS_LOGS}:/host:ro,rslave"
    ports:
      - "9100:9100"
    networks:
      - shared-monitoring-net
    restart: unless-stopped
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "1"
          memory: "128m"

  mongo-db-exporter:
    image: percona/mongodb_exporter:2.37
    container_name: prometheus-mongo-db-exporter
    command:
      - "--mongodb.global-conn-pool"
      - "--collector.diagnosticdata"
      - "--discovering-mode"
      - "--compatible-mode"
      - "--mongodb.collstats-colls=admin.companies,admin.restaurants"
      - "--collect-all"
    environment:
      - MONGODB_URI=mongodb://mongoadmin:${MONGO_DB_PASSWORD}@shared-mongo-db:27017/?maxPoolSize=1000&waitQueueTimeoutMS=15000
    restart: unless-stopped
    ports:
      - "9216:9216"
      - "17001:17001"
    networks:
      - mongo-network
      - shared-monitoring-net
    depends_on:
      - prometheus
      - shared-mongo-db
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "1"
          memory: "128m"

  kafka-exporter:
    image: danielqsj/kafka-exporter:v1.9.0
    container_name: prometheus-kafka-exporter
    command:
      - "--kafka.server=kafka:29092"
    ports:
      - 9308:9308
    networks:
      - kafka-net
      - shared-monitoring-net
    depends_on:
      - prometheus
      - kafka
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 30
        delay: 15s
        window: 1080s
      resources:
        limits:
          cpus: "1"
          memory: "128m"

  mssql-exporter:
    image: allmantool/sql-monitoring-exporter:0.0.8
    container_name: prometheus-mssql-exporter
    environment:
      SERVER: ${SQL_SERVER}
      PORT: ${SQL_SERVER_2025_PORT}
      USERNAME: SA
      PASSWORD: ${WMS_SQL_PASSWORD}
    ports:
      - 4000:4000
    networks:
      - mssql-network
      - shared-monitoring-net
    depends_on:
      - prometheus
      - ${SQL_SERVER}
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "0.5"
          memory: "256m"

  redis-exporter:
    image: oliver006/redis_exporter:v1.80.1
    container_name: prometheus-redis-exporter
    environment:
      REDIS_ADDR: redis_server:6379
    ports:
      - 9121:9121
    networks:
      - redis-network
      - shared-monitoring-net
    depends_on:
      - prometheus
      - redis_server
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "1"
          memory: "128m"

  nginx-exporter:
    image: nginx/nginx-prometheus-exporter:1.5.1
    container_name: nginx-exporter
    command:
      - "--nginx.scrape-uri=http://localhost:5407/stub_status"
    ports:
      - 9113:9113
    networks:
      - ui-network
    depends_on:
      - prometheus
      - homebudget-ui
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "1"
          memory: "128m"

  event-store-db-exporter:
    image: marcinbudny/eventstore_exporter:0.17.0
    container_name: event-store-db-exporter
    environment:
      - EVENTSTORE_URL=http://event-store-db:2113
      - EVENTSTORE_USER=admin
      - EVENTSTORE_PASSWORD=changeit
      - ENABLE_PARKED_MESSAGES_STATS=True
    ports:
      - 9448:9448
    networks:
      - event-store-db-net
      - shared-monitoring-net
    depends_on:
      - prometheus
      - event-store-db
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 10
        delay: 5s
        window: 180s
      resources:
        limits:
          cpus: "1"
          memory: "128m"
